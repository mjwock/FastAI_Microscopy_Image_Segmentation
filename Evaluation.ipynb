{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Evaluation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNAEmNhu7b2i3d7H0XPqRhH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VxndeHSnHNuJ","colab_type":"code","colab":{}},"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    #!git clone https://github.com/mjwock/DeepFLaSH_Pytorch.git /content/drive/My\\ Drive/DeepFLaSH_Pytorch/FastAI/\n","    %cd /content/drive/My\\ Drive/DeepFLaSH_Pytorch/FastAI2\n","    #!git pull\n","except:\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zY1b27AcQr3I","colab_type":"code","outputId":"184ce6b6-c34a-4989-c310-96831028f03d","executionInfo":{"status":"ok","timestamp":1587739902107,"user_tz":-120,"elapsed":7670,"user":{"displayName":"Marlo Jwock","photoUrl":"","userId":"07321770184388401262"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["%load_ext autoreload\n","%autoreload 2\n","%matplotlib inline \n","\n","!pip install elasticdeform\n","\n","import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from pathlib import Path\n","\n","from torch import nn\n","from torchsummary import summary\n","from sklearn.model_selection import StratifiedKFold, KFold\n","\n","from datetime import datetime\n","\n","#imports for Tile\n","from tqdm import tqdm\n","from math import ceil\n","\n","from deepflash import preproc, unetadaption, utility\n","from deepflash.fastai_extension import *"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n","Requirement already satisfied: elasticdeform in /usr/local/lib/python3.6/dist-packages (0.4.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from elasticdeform) (1.18.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from elasticdeform) (1.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SeO8nKAnQ0ze","colab_type":"code","colab":{}},"source":["MODEL_DIR = '/content/drive/My Drive/DeepFLaSH_Pytorch/FastAI2/data/model'\n","\n","TEST_IMAGES_DIR = '/content/drive/My Drive/DeepFLaSH_Pytorch/FastAI2/data/images/red/'\n","LABEL_DIR = '/content/drive/My Drive/DeepFLaSH_Pytorch/FastAI2/data/temp_data/labels/'\n","\n","IMAGE_TYPE = 'L'    # 'L' for greyscale, 'RGB'for color, 'P' for palette images\n","\n","TILE_SHAPE = (540,540)    # desired input size\n","MASK_SHAPE = (356,356)       # 540-184\n","\n","SEED = 42"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eIkqyc28bOif","colab_type":"text"},"source":["Model"]},{"cell_type":"code","metadata":{"id":"A-2leGqPbRuc","colab_type":"code","colab":{}},"source":["model_id = '201903-0954'\n","model_name = 'final_model.pkl'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nRsRs-ntZ4hi","colab_type":"text"},"source":["##Load Model"]},{"cell_type":"code","metadata":{"id":"OKKSKNKnZ0CL","colab_type":"code","colab":{}},"source":["learn = load_learner(f'{MODEL_DIR}/{model_id}',model_name)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-WYIvSue4oU","colab_type":"code","colab":{}},"source":["data = (CustomSegmentationItemList.from_folder(TEST_IMAGES_DIR, convert_mode=IMAGE_TYPE))\n","labels = (CustomSegmentationLabelList.from_folder(LABEL_DIR, convert_mode='L'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lazk9HPDZXXF","colab_type":"text"},"source":["##**TileGenerator**"]},{"cell_type":"code","metadata":{"id":"EMFXlkuCZY2p","colab_type":"code","colab":{}},"source":["class TileGenerator:\n","  '''\n","  TileGenerator to split input images into multiple tiles to be used by learner. \n","  Images can be padded to get a prediction of complete image\n","  '''\n","  def __init__(\n","      self,\n","      data,\n","      learner,\n","      tile_shape=(540,540),\n","      mask_shape=(356,356),\n","      same_size = True,\n","      padding_mode = None   # None, 'zeros', 'border' or 'reflection'\n","  ):\n","    \n","    self.data = [image for image in tqdm(data,desc='Loading Data: ')]\n","\n","    self.learner = learner\n","    self.tile_shape = tile_shape\n","    self.mask_shape = mask_shape\n","    self.same_size = same_size\n","    \n","    if padding_mode:\n","      padding = tuple(np.subtract(tile_shape,mask_shape)//2)\n","      self.pad_images(padding, padding_mode)\n","    \n","    self.data_shapes = [self.data[0].shape]*len(data) if same_size else [item.shape for item in self.data]\n","\n","    self.tiles = None\n","    self.tile_dimensions = None\n","    self.predictions = None\n","    self.split_tiles()\n","\n","  def __len__(self):\n","      return len(self.data)\n","\n","  def __getitem__(self, index):\n","      if self.tiles == None:\n","        return self.data[index]\n","      else:\n","        return self.tiles[index]\n","\n","  def tile_splitter(self,input_shape:tuple):\n","    \"\"\"\n","    Gets the pixelwise regions of the tiles for cropping the image. Tiles need \n","    to overlap, if your model doesn't use padding to compensate the cropping of \n","    Convolutions.\n","    :param input_shape: Input shape of the Image in form of (C,H,W)\n","\n","    :return tiles: pixelwise tile areas (xs,xf,ys,yf) with S(xs,ys) being left \n","    upper corner and F(xf,yf) being the lower right corner of our rectangular tile\n","    :return tile_dimensions: decimal numbers of how many tiles are needed for given\n","    input image shape in form of (xtiles,ytiles)\n","    \"\"\"\n","    tx, ty = self.tile_shape\n","    mx, my = self.mask_shape\n","    px, py = np.subtract((tx,ty),(mx,my)) # x and y padding\n","\n","    _, dx, dy = input_shape\n","\n","    # how many tiles are needed (decimal precision)\n","    xtiles = (dx-px)/mx \n","    ytiles = (dy-py)/my \n","\n","    # add starting points for full tiles with spacing mx and my\n","    x_start = [0+mx*ix for ix in range(int(xtiles))]\n","    y_start = [0+my*iy for iy in range(int(ytiles))]\n","\n","    # add a last x, y starting point for none-integer tiles\n","    if not xtiles%1==0:\n","      x_start.append(dx-tx)\n","\n","    if not ytiles%1==0:\n","      y_start.append(dy-ty)\n","\n","    # build tiles with width tx and height ty\n","    tiles = []\n","    for y in y_start:\n","      for x in x_start:\n","        tiles.append((x,x+tx,y,y+ty))\n","\n","    tile_dimensions = (xtiles,ytiles)\n","    \n","    return tiles, tile_dimensions\n","\n","  def split_tiles(self):\n","    '''\n","    Splits the images into tiles and saves them to 'self.tiles' as well as the float \n","    precision of the needed amount of tiles into 'self.tile_dimensions' in form of \n","    a list of tuples (xtiles,ytiles)\n","    '''\n","    ds = self.data_shapes\n","\n","    # if all input images have the same shape\n","    if self.same_size:\n","      \n","      # call tile_splitter to split all images depending on their shape\n","      tile_regions, tile_dimension =  self.tile_splitter(ds[0])\n","\n","      tiles = []\n","\n","      for img in tqdm(self.data,desc='Building Tiles'):\n","        img_tiles = []\n","        for region in tile_regions:\n","          img_tiles.append(self.crop_to_tile(img,region))\n","        tiles.append(img_tiles)\n","\n","      self.tiles = tiles\n","      self.tile_dimensions = [tile_dimension]*len(self.data)\n","    \n","    #if input images have different shapes\n","    else:\n","      \n","      tiles = []\n","      tile_dimensions =[]\n","\n","      for i, img in enumerate(self.data,desc='Building Tiles'):\n","        \n","        # call tile_splitter to split image depending on image shape\n","        tile_regions, tile_dimension =  self.tile_splitter(ds[i])\n","        img_tiles = []\n","\n","        for region in tile_regions:\n","          img_tiles.append(self.crop_to_tile(img, region))\n","\n","        tiles.append(img_tiles)\n","        tile_dimensions.append(tile_dimension)\n","\n","      self.tiles = tiles\n","      self.tile_dimensions = tile_dimensions\n","\n","  def crop_to_tile(self, img:Image, region):\n","    '''\n","    Crops input image to rectangular region from corner (xs,ys) to corner (xf,yf)\n","      :param img: The Image to be cropped\n","      :param region: tuple defining pixels for cropped region in form (xs,xf,ys,yf),\n","      s denoting the starting pixels and f denoting the final pixels\n","\n","      :return: Cropped Image\n","    '''\n","    xs,xf,ys,yf = region\n","    return Image(img.data[:,ys:yf,xs:xf])\n","  \n","  def pad_images(self,padding,padding_mode): \n","    '''\n","    Pads all images in self.data \n","      :param padding: magnitude of padding\n","    '''\n","    assert padding[0] == padding[1], 'For padding: tile_shape and mask_shape need to be squares.'\n","    self.data = [image.pad(padding[0],padding_mode) for image in tqdm(self.data,desc='Padding images')]\n","\n","  def display_tiles(self,batch:Iterator,shape:tuple=(3,3),figsize:tuple=(9,9)):\n","    '''\n","    Displays given tiles in 'batch' in subplots arranged by 'shape' within figure\n","    with figsize of 'figsize'\n","    :param batch: list of Images\n","    :param shape: rows and colums of subplots (tuple)\n","    :param figsize: figsize of plt figure\n","    '''\n","    f, axarr = plt.subplots(shape[0], shape[1], figsize=figsize)\n","    f.tight_layout(pad = 0)\n","\n","    for ax,tile in zip(axarr.flatten(), batch):\n","      plt.subplots_adjust(wspace=0, hspace=0)\n","      ax.set_xticklabels([])\n","      ax.set_yticklabels([])\n","      tile.show(ax)\n","\n","  def stitch_helper(self,dimensions,mask_shape,reshape = True):\n","    '''\n","    creates a lookup matrix on how the tiles need to be cropped for stitching.\n","      :args dimensions: 'tile_dimension' on how many tiles were needed for this \n","                        image with float precision (N,M)\n","      :args mask_shape: the shape of the prediction output (W,H)\n","      :args reshap: reshape to an indexable array with length NxM\n","\n","      :return: lookup matrix (np.array (NxM,R) or (N,M,R)) with R being the region \n","               argument for crop_to_tile()\n","    '''\n","    xd,yd = dimensions\n","    xs,ys = mask_shape\n","    rows  = ceil(xd)*ceil(yd)\n","    lookup_matrix = np.ones((ceil(xd),ceil(yd),4))*(0,xs,0,ys)\n","    \n","    for ix in range(ceil(xd)):\n","      xd -= 1\n","      if xd<0:\n","        lookup_matrix[ix,:,2] = -xd*xs\n","      else:\n","        lookup_matrix[ix,:,2] = 0\n","    \n","    for iy in range(ceil(yd)):\n","      yd -= 1\n","      if yd<0:\n","        lookup_matrix[:,iy,0] = -yd*ys\n","      else:\n","        lookup_matrix[:,iy,0] = 0\n","    if reshape:\n","      return np.reshape(lookup_matrix,(rows,4)).astype(int)\n","    else:\n","      return lookup_matrix.astype(int)\n","\n","  def show_tiles(self, index, crop_to = 'mask', base_size=3):\n","    '''\n","    Displays tiles on a set of subplots.\n","      :param index: index of instance to be displayed (int)\n","      :param crop_to: crop is either None, 'mask' or 'original' {None,'mask','original'}\n","      :base_size: base size that each tile\n","    '''\n","    batch = copy(self.tiles[index])\n","    dimensions = self.tile_dimensions[index]\n","    x, y = np.ceil(dimensions).astype(int)\n","    shape = (x,y)\n","    figsize = (x*base_size,y*base_size)\n","\n","    if crop_to == 'mask':\n","      for tile in batch:\n","        tile = tile.crop(self.mask_shape)\n","\n","    elif crop_to == 'original':  \n","      lookup_matrix = self.stitch_helper(dimensions,self.mask_shape)  \n","\n","      for region, tile in zip(lookup_matrix,batch):\n","        tile = tile.crop(self.mask_shape) \n","        tile = self.crop_to_tile(tile,region)\n","\n","    self.display_tiles(batch,shape,figsize)\n","\n","  def show_predictions(self):\n","    print('Not implemented')\n","\n","  def predict_instance(self, x:Image):\n","    '''\n","    Calls FastAIs Learner.predict() function and transforms it into a prediction \n","    mask.\n","    :args x: tile to be predicted (Image)\n","\n","    :return: prediction (Image)\n","    :return: probabilities (Tensor)\n","    '''\n","    raw = self.learner.predict(x)\n","    probabilities, values = raw[1].max(0)\n","\n","    return Image(values.unsqueeze(0)), probabilities.unsqueeze(0)\n","\n","  def predict_all(self):\n","    '''\n","    Predicts all tiles from all images by calling predict_instance() and saves \n","    them together with the pixelwise probabilities to self.predictions.\n","    '''\n","    solutions = []\n","    probabilities = []\n","    \n","    # iterate all images\n","    for batch in tqdm(self.tiles,desc='Predicting Tiles'):\n","      batch_solutions = []\n","      batch_probabilities = []\n","\n","      # iterate all tiles in image and predict\n","      for tile in batch:\n","\n","        tile_prediction, tile_probabilities = self.predict_instance(tile)\n","        batch_solutions.append(tile_prediction)\n","        batch_probabilities.append(tile_probabilities)\n","      solutions.append(batch_solutions)\n","      probabilities.append(batch_probabilities)\n","\n","    self.predictions = [solutions,probabilities]\n","  \n","  def stitch_image(self,tile_list,lookup_matrix,is_img=True):\n","    '''\n","    Stitch image by concatenating and cropping tiles, depending on lookup_matrix\n","      :args tile_list: list of tiles making out image\n","      :args lookup_matrix: list\n","\n","      :return: concatenated Image \n","    '''\n","\n","    image = None\n","    \n","    for brow,lmrow in zip(tile_list, lookup_matrix):\n","\n","      row = None\n","      for tile, region in zip(brow,lmrow):\n","\n","        # crops tile region if region of interest is smaller than given input\n","        if not np.array_equal(region,[0,self.mask_shape[0],0,self.mask_shape[1]]):\n","          if is_img:\n","            tile = self.crop_to_tile(tile, region)\n","          else:\n","            xs,xf,ys,yf = region\n","            tile = tile[:,ys:yf,xs:xf]\n","        \n","        #concat columns\n","\n","        if row is None:\n","          row = tile.data if is_img else tile\n","        else:\n","          row = torch.cat((row,tile.data if is_img else tile),2)\n","\n","      #concat rows\n","      if image is None:\n","        image = row\n","      else:\n","        image = torch.cat((image,row),1)\n","    \n","    return Image(image) if is_img else image\n","\n","  def stitch_results(self):\n","    '''\n","    Stitches the results in self.predictions\n","    '''\n","    \n","    xs,ys = self.mask_shape\n","    results = []\n","    for tiles, probabilities, dimensions in zip(self.predictions[0],self.predictions[1],self.tile_dimensions):\n","      xd,yd = np.ceil(dimensions)\n","      lookup_matrix = self.stitch_helper(dimensions,(xs,ys),reshape=False) \n","      _reshape = lookup_matrix.shape[0:2]\n","\n","      tiles_array = np.reshape(tiles,_reshape) \n","      prob_array = [probabilities[i:i+xd] for i in range(0, len(data_list), 3)]\n","      \n","      image = self.stitch_image(tiles_array,lookup_matrix)\n","      prob_map = self.stitch_image(prob_array,lookup_matrix,is_img=False)\n","\n","      results.append(image,prob_map)\n","\n","    return results"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BamC883uYOjR","colab_type":"code","outputId":"9e9f6ea4-9f88-43f0-fce4-e9ec2875c87b","executionInfo":{"status":"ok","timestamp":1587739903705,"user_tz":-120,"elapsed":6926,"user":{"displayName":"Marlo Jwock","photoUrl":"","userId":"07321770184388401262"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["thisTile = TileGenerator(data,learn, TILE_SHAPE,MASK_SHAPE,same_size=True,padding_mode='reflection')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading Data: 100%|██████████| 36/36 [00:00<00:00, 65.15it/s]\n","Padding images: 100%|██████████| 36/36 [00:00<00:00, 204.86it/s]\n","Building Tiles: 100%|██████████| 36/36 [00:00<00:00, 5896.17it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-nCrID0XmOD-","colab_type":"code","outputId":"8e3a41fa-5860-44a8-c10d-22a76ba5da28","executionInfo":{"status":"ok","timestamp":1587740005153,"user_tz":-120,"elapsed":108090,"user":{"displayName":"Marlo Jwock","photoUrl":"","userId":"07321770184388401262"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["thisTile.predict_all()\n","predictions = thisTile.predictions\n","#thisTile.predictions = predictions"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Predicting Tiles: 100%|██████████| 36/36 [01:40<00:00,  2.80s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"R9irkX0DCLaY","colab_type":"code","outputId":"8df31817-d742-41eb-ae7b-01677d8ba436","executionInfo":{"status":"error","timestamp":1587740005156,"user_tz":-120,"elapsed":107781,"user":{"displayName":"Marlo Jwock","photoUrl":"","userId":"07321770184388401262"}},"colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["thisTile.stitch_results()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-2b49f7e32fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthisTile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstitch_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-18-62326d096c70>\u001b[0m in \u001b[0;36mstitch_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m       \u001b[0mtiles_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_reshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m       \u001b[0mprob_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mxd\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstitch_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiles_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlookup_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data_list' is not defined"]}]},{"cell_type":"code","metadata":{"id":"fU10-SilPdq9","colab_type":"code","colab":{}},"source":["      rows = None\n","      for brow,lmrow in zip(batch_array, lookup_matrix):\n","\n","        row = None\n","        for tile, region in zip(brow,lmrow):\n","\n","          # crops tile region if region of interest is smaller than given input\n","          if not np.array_equal(region,[0,xs,0,ys]):\n","            print(f'Region: {region}, ({type(region)})')\n","            tile = self.crop_to_tile(tile, region)\n","          \n","          #concat columns\n","          if row is None:\n","            row = tile.data\n","          else:\n","            row = torch.cat((row,tile.data),2)\n","\n","        #concat rows\n","        if rows is None:\n","          rows = row\n","        else:\n","          rows = torch.cat((rows,row),1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RtLhQpqGCsKL","colab_type":"code","colab":{}},"source":["thisTile.show_tiles(1,crop_to='original')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hRBye9y9mVMN","colab_type":"code","colab":{}},"source":["a[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPG_hpiHPUT6","colab_type":"code","colab":{}},"source":["ax.add_patch(copy(bbox))\n","                        #(x-offset,y-offset)      ,width      ,height\n","bbox = patches.Rectangle((padding-1,padding-1),wdt-2*padding,hgt-2*padding,edgecolor='r',linewidth=1,facecolor='none')\n","\n","pil2tensor(x,np.float32)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jr5rFY4gbMx9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}